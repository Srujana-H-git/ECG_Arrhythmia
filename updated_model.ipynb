import os
import wfdb
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, f1_score

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback
from tensorflow.keras.utils import to_categorical

# === PARAMETERS ===
record_list = ['100', '101', '102', '103', '104']  # MIT-BIH records
segment_length = 180
data_path = 'ecg_data/'
save_model_path = 'final_model.h5'
os.makedirs(data_path, exist_ok=True)

# === AUTO-DOWNLOAD RECORDS ===
def download_records_if_missing():
    for record in record_list:
        hea_path = os.path.join(data_path, f"{record}.hea")
        if not os.path.exists(hea_path):
            print(f"üì• Downloading record {record}...")
            wfdb.dl_database('mitdb', dl_dir=data_path, records=[record])
    print("‚úÖ All records available.\n")

# === LOAD & PREPROCESS DATA ===
def load_data():
    X = []
    y = []
    for record_name in record_list:
        record = wfdb.rdrecord(os.path.join(data_path, record_name))
        annotation = wfdb.rdann(os.path.join(data_path, record_name), 'atr')
        signal = record.p_signal[:, 0]
        r_peaks = annotation.sample
        labels = annotation.symbol

        for i, r in enumerate(r_peaks):
            label = labels[i]
            if label in ['N', 'L', 'R', 'A', 'V']:  # Selected classes
                start = r - segment_length // 2
                end = r + segment_length // 2
                if start >= 0 and end < len(signal):
                    segment = signal[start:end]
                    X.append(segment)
                    y.append(label)

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    X = np.array(X)
    y = np.array(y_encoded)

    class_map = {i: label for i, label in enumerate(le.classes_)}
    with open('class_mapping.json', 'w') as f:
        json.dump(class_map, f)

    return X[..., np.newaxis], to_categorical(y), len(le.classes_), le

# === BUILD CNN MODEL ===
def build_model(input_shape, num_classes):
    model = Sequential([
        Conv1D(32, kernel_size=5, activation='relu', input_shape=input_shape),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        Dropout(0.3),

        Conv1D(64, kernel_size=5, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        Dropout(0.3),

        Conv1D(128, kernel_size=5, activation='relu'),
        BatchNormalization(),
        MaxPooling1D(pool_size=2),
        Dropout(0.3),

        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# === CUSTOM CALLBACK TO TRACK F1-SCORE ===
class F1ScoreTracker(Callback):
    def __init__(self, X_val, y_val):
        super().__init__()
        self.X_val = X_val
        self.y_val = y_val
        self.f1_scores = []

    def on_epoch_end(self, epoch, logs=None):
        y_pred = self.model.predict(self.X_val)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_true = np.argmax(self.y_val, axis=1)
        f1 = f1_score(y_true, y_pred_classes, average='macro')
        self.f1_scores.append(f1)
        print(f"üîπ Epoch {epoch + 1} - Val F1 Score: {f1:.4f}")

# === RISK ANALYTICS FUNCTIONS ===
def generate_patient_metadata(num_samples):
    np.random.seed(42)
    ages = np.random.randint(30, 80, size=num_samples)
    genders = np.random.choice(['M', 'F'], size=num_samples)
    bp = np.random.randint(100, 180, size=num_samples)
    cholesterol = np.random.randint(150, 280, size=num_samples)
    diabetic = np.random.choice([0, 1], size=num_samples, p=[0.7, 0.3])
    return list(zip(ages, genders, bp, cholesterol, diabetic))

def compute_risk_scores(metadata, y_preds):
    risk_levels = []
    for i, (age, gender, bp, chol, dia) in enumerate(metadata):
        score = 0
        if age > 60: score += 1
        if bp > 140: score += 1
        if chol > 240: score += 1
        if dia: score += 1
        if y_preds[i] in [1, 3, 4]:  # L, A, V
            score += 1
        risk_levels.append("High" if score >= 3 else "Moderate" if score == 2 else "Low")
    return risk_levels

# === TRAIN, EVALUATE & PLOT ===
def train_and_evaluate(X, y, num_classes, label_encoder):
    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y.argmax(axis=1), random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp.argmax(axis=1), random_state=42)

    model = build_model(X_train.shape[1:], num_classes)

    checkpoint = ModelCheckpoint(save_model_path, monitor='val_accuracy', save_best_only=True)
    early_stop = EarlyStopping(patience=5, restore_best_weights=True)
    f1_tracker = F1ScoreTracker(X_val, y_val)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=10,
        batch_size=64,
        callbacks=[checkpoint, early_stop, f1_tracker],
        verbose=1
    )

    print("\n‚úÖ Final Evaluation:")
    loss, acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Accuracy: {acc:.4f}, Loss: {loss:.4f}")

    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)

    int_to_class = {i: label for i, label in enumerate(label_encoder.classes_)}

    print("\nüìä Classification Report:")
    print(classification_report(y_true, y_pred_classes, target_names=[int_to_class[i] for i in sorted(int_to_class)]))

    # === Risk Profiling ===
    metadata = generate_patient_metadata(len(y_true))
    risk_levels = compute_risk_scores(metadata, y_pred_classes)

    print("\nü©∫ Sample Risk Profiles:")
    for i in range(5):
        print(f"Patient {i+1}: Age={metadata[i][0]}, BP={metadata[i][2]}, Chol={metadata[i][3]}, Diabetic={metadata[i][4]} ‚Üí Risk={risk_levels[i]}")

    # === Risk Level Plot ===
    risk_counts = {'Low': 0, 'Moderate': 0, 'High': 0}
    for r in risk_levels:
        risk_counts[r] += 1

    plt.figure(figsize=(6, 4))
    sns.barplot(x=list(risk_counts.keys()), y=list(risk_counts.values()), palette='coolwarm')
    plt.title("Risk Level Distribution")
    plt.tight_layout()
    plt.savefig("risk_distribution.png")
    plt.show()

    # === Visualization ===
    cm = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[int_to_class[i] for i in sorted(int_to_class)],
                yticklabels=[int_to_class[i] for i in sorted(int_to_class)])
    plt.title("Confusion Matrix")
    plt.savefig("confusion_matrix.png")
    plt.show()

    plt.figure()
    plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
    plt.title('Accuracy per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("accuracy_vs_epoch.png")
    plt.show()

    plt.figure()
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
    plt.title('Loss per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("loss_vs_epoch.png")
    plt.show()

    plt.figure()
    plt.plot(f1_tracker.f1_scores, label='Validation F1 Score', marker='o', color='green')
    plt.title('F1 Score per Epoch')
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("f1_score_vs_epoch.png")
    plt.show()

# === MAIN RUNNER ===
if __name__ == "__main__":
    print("üîÅ Checking for ECG records...")
    download_records_if_missing()

    print("üöÄ Loading and preprocessing data...")
    X, y, num_classes, label_encoder = load_data()
    print(f"‚úÖ Data loaded: {X.shape[0]} samples, {num_classes} classes")

    print("üß† Training CNN model...")
    train_and_evaluate(X, y, num_classes, label_encoder)
